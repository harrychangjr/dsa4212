{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35b45b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2343 sha256=619174b376d7be0d2d578cd1015bbd0c0503ce509f111325679ab6f4924a0e8a\n",
      "  Stored in directory: /Users/harrychang/Library/Caches/pip/wheels/14/25/f7/1cc0956978ae479e75140219088deb7a36f60459df242b1a72\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0.post1\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.1-cp38-cp38-macosx_10_9_x86_64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.7.0 in /Users/harrychang/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/harrychang/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.23.5)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.3.1 smart-open-6.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70cabc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "def preprocess_newsgroups(newsgroups_data):\n",
    "    sentences = [document.split('\\n') for document in newsgroups_data.data]\n",
    "    sentences = [[sentence.split() for sentence in document] for document in sentences]\n",
    "    return [sentence for document in sentences for sentence in document]\n",
    "\n",
    "sentences = preprocess_newsgroups(newsgroups)\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4, sg=1)\n",
    "model.save(\"newsgroups_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6006595f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'computer':\n",
      "workstation - 0.77\n",
      "lab - 0.76\n",
      "bulletin - 0.75\n",
      "implementing - 0.73\n",
      "packet - 0.73\n"
     ]
    }
   ],
   "source": [
    "def find_similar_words(model, word, topn=5):\n",
    "    return model.wv.most_similar(word, topn=topn)\n",
    "\n",
    "word2vec_model = Word2Vec.load(\"newsgroups_word2vec.model\")\n",
    "word = \"computer\"\n",
    "similar_words = find_similar_words(word2vec_model, word)\n",
    "print(f\"Similar words to '{word}':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word} - {similarity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe38c8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5655dd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(document, model):\n",
    "    doc_vec = np.zeros(model.vector_size)\n",
    "    word_count = 0\n",
    "    for word in document:\n",
    "        if word in model.wv:\n",
    "            doc_vec += model.wv[word]\n",
    "            word_count += 1\n",
    "    if word_count == 0:\n",
    "        return doc_vec\n",
    "    return doc_vec / word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "957c6ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_word2vec_model = Word2Vec.load(\"newsgroups_word2vec.model\")\n",
    "\n",
    "articles = newsgroups.data\n",
    "labels = newsgroups.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85df1662",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([document_vector(article.split(), newsgroups_word2vec_model) for article in articles])\n",
    "y = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f51844a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d285969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrychang/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "938db1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4607427055702918\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a04b3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = ['good', 'excellent', 'amazing', 'positive', 'happy']\n",
    "negative_words = ['bad', 'terrible', 'awful', 'negative', 'unhappy']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a0d0dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_sentiment(document):\n",
    "    sentiment_score = 0\n",
    "    for word in positive_words:\n",
    "        sentiment_score += document.count(word)\n",
    "    for word in negative_words:\n",
    "        sentiment_score -= document.count(word)\n",
    "    return 1 if sentiment_score > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dcef4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_labels = [assign_sentiment(article) for article in articles]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef6ea07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, sentiment_labels, test_size=0.2)\n",
    "\n",
    "sentiment_classifier = LogisticRegression()\n",
    "sentiment_classifier.fit(X_train, y_train)\n",
    "y_pred = sentiment_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9531f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment accuracy: 0.8636604774535809\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Sentiment accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53baf12d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
